{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os  \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepath(f):\n",
    "    d = os.path.join(os.path.dirname(os.getcwd()), 'processed_data', f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training and test datasets\n",
    "train_data=pd.read_csv(filepath(\"final_training_set.csv\"))\n",
    "#test_data=pd.read_csv(filepath(\"final_test_set.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     345415\n",
       "Yes    212796\n",
       "Name: PotentialFraud, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"PotentialFraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider\n",
      "PotentialFraud\n",
      "InscClaimAmtReimbursed\n",
      "DeductibleAmtPaid\n",
      "inpatient\n",
      "Gender\n",
      "Race\n",
      "RenalDiseaseIndicator\n",
      "ChronicCond_Alzheimer\n",
      "ChronicCond_Heartfailure\n",
      "ChronicCond_KidneyDisease\n",
      "ChronicCond_Cancer\n",
      "ChronicCond_ObstrPulmonary\n",
      "ChronicCond_Depression\n",
      "ChronicCond_Diabetes\n",
      "ChronicCond_IschemicHeart\n",
      "ChronicCond_Osteoporasis\n",
      "ChronicCond_rheumatoidarthritis\n",
      "ChronicCond_stroke\n",
      "IPAnnualReimbursementAmt\n",
      "IPAnnualDeductibleAmt\n",
      "OPAnnualReimbursementAmt\n",
      "OPAnnualDeductibleAmt\n",
      "is_alive\n",
      "age\n",
      "claim_duration\n",
      "time_under_care\n",
      "admitDiagInFinalDiagnosis\n",
      "diagnosis_1\n",
      "diagnosis_2\n",
      "diagnosis_3\n",
      "diagnosis_4\n",
      "diagnosis_5\n",
      "diagnosis_6\n",
      "diagnosis_7\n",
      "diagnosis_8\n",
      "diagnosis_9\n",
      "diagnosis_10\n",
      "diagnosis_11\n",
      "diagnosis_12\n",
      "diagnosis_13\n",
      "diagnosis_14\n",
      "diagnosis_15\n",
      "diagnosis_16\n",
      "diagnosis_17\n",
      "diagnosis_18\n",
      "diagnosis_19\n",
      "procedure_1\n",
      "procedure_2\n",
      "procedure_3\n",
      "procedure_4\n",
      "procedure_5\n",
      "procedure_6\n",
      "procedure_7\n",
      "procedure_8\n",
      "procedure_9\n",
      "procedure_10\n",
      "procedure_11\n",
      "procedure_12\n",
      "procedure_13\n",
      "procedure_14\n",
      "procedure_15\n",
      "procedure_16\n",
      "procedure_17\n",
      "procedure_18\n",
      "procedure_1 has been removed as it is constant\n",
      "procedure_2 has been removed as it is constant\n",
      "procedure_3 has been removed as it is constant\n"
     ]
    }
   ],
   "source": [
    "def filepath(f):\n",
    "    d = os.path.join(os.path.dirname(os.getcwd()), 'processed_data', f)\n",
    "    return d\n",
    "#Import training and test datasets\n",
    "train_data=pd.read_csv(filepath(\"final_training_set.csv\"))\n",
    "test_data=pd.read_csv(filepath(\"final_test_set.csv\"))\n",
    "train_data\n",
    "uid = train_data[\"ClaimID\"]\n",
    "train_data = train_data.drop(\n",
    "    [\"ClaimID\"],\n",
    "    axis = 1\n",
    ")\n",
    "for c in train_data.columns:\n",
    "    print(c)\n",
    "def minmax_encode(df, col):\n",
    "    \"\"\"\n",
    "    Return dataset including the minmax encoded column and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be numeric\n",
    "    \"\"\"\n",
    "\n",
    "    maxx = df[col].max()\n",
    "    minx = df[col].min()\n",
    "    out = list(map(lambda x: (x-minx)/(maxx-minx), df[col]))\n",
    "    new_colname = col + \"_minmax\"\n",
    "    df[new_colname] = out\n",
    "    return df.drop(\n",
    "        [col],\n",
    "        axis = 1\n",
    "    )\n",
    "def one_hot_encode(df, col):\n",
    "    \"\"\"\n",
    "    Returns the dataset including the one hot encoded columns and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be a Series that represents a categorical variable\n",
    "    \"\"\"\n",
    "    ohe_cols = pd.get_dummies(df[col], prefix = col)\n",
    "    output = pd.concat(\n",
    "        [df, ohe_cols],\n",
    "        axis = 1,\n",
    "    ).drop(\n",
    "        [col],\n",
    "        axis = 1\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def frequency_encode(df, col):\n",
    "    \"\"\"\n",
    "    Returns the dataset including the frequency encoded column and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be a Series that represents a categorical variable with high cardinality\n",
    "    \"\"\"\n",
    "    val_counts = df[col].value_counts().to_dict()\n",
    "    total = len(col)\n",
    "    out = []\n",
    "    for x in df[col]:\n",
    "        out.append(val_counts[x]/total)\n",
    "    new_colname = col + '_freq'\n",
    "    df[new_colname] = out\n",
    "    df.drop(\n",
    "        [col],\n",
    "        axis = 1,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return df\n",
    "freq_encoded_cols = []\n",
    "ohe_cols = []\n",
    "num_cols = []\n",
    "unique_threshold = 30\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].nunique() == 1:\n",
    "        print(col,\"has been removed as it is constant\")\n",
    "        train_data.drop([col], axis=1, inplace=True)\n",
    "    elif train_data[col].nunique() == 2: # Binary columns\n",
    "        continue\n",
    "    elif train_data[col].dtype in ['int64','float64']:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "        num_cols.append(col)\n",
    "    elif train_data[col].nunique() > unique_threshold:\n",
    "        freq_encoded_cols.append(col)\n",
    "    elif 2 < train_data[col].nunique() <= unique_threshold:\n",
    "        ohe_cols.append(col)\n",
    "        \n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col in num_cols:\n",
    "        train_data = minmax_encode(train_data, col)\n",
    "    elif col in ohe_cols:\n",
    "        train_data = one_hot_encode(train_data, col)\n",
    "    elif col in freq_encoded_cols:\n",
    "        try:\n",
    "            train_data = frequency_encode(train_data, col)\n",
    "        except:\n",
    "            print(col)\n",
    "    elif train_data[col].nunique() == 1:\n",
    "        train_data.drop(\n",
    "            [col],\n",
    "            axis = 1,\n",
    "            inplace = True\n",
    "        )\n",
    "    else:\n",
    "        train_data[col] = train_data[col].astype('bool')\n",
    "y = train_data[\"PotentialFraud\"]\n",
    "train_data.drop(\n",
    "    [\"PotentialFraud\"],\n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Isolation Forest model\n",
    "model = IsolationForest(contamination=0.2,random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "model.fit(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_iso = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_iso = [True if label == -1 else False for label in y_pred_iso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6143958868894601\n",
      "F1 Score: 0.33634457667879386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.83      0.73     69258\n",
      "        True       0.49      0.26      0.34     42385\n",
      "\n",
      "    accuracy                           0.61    111643\n",
      "   macro avg       0.57      0.55      0.53    111643\n",
      "weighted avg       0.59      0.61      0.58    111643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaulating Isolation Forest accuracy\n",
    "# Evaluate the model on training data\n",
    "iso_accuracy = accuracy_score(y_valid, y_pred_iso)\n",
    "iso_f1_score = f1_score(y_valid,y_pred_iso)\n",
    "iso_report = classification_report(y_valid, y_pred_iso)\n",
    "\n",
    "print(f'Accuracy: {iso_accuracy}')\n",
    "print(f'F1 Score: {iso_f1_score}')\n",
    "print(iso_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "log_y_pred = logistic_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.818815331010453\n",
      "F1 Score: 0.7341359549971084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.92      0.86     69258\n",
      "        True       0.83      0.66      0.73     42385\n",
      "\n",
      "    accuracy                           0.82    111643\n",
      "   macro avg       0.82      0.79      0.80    111643\n",
      "weighted avg       0.82      0.82      0.81    111643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_train_accuracy = accuracy_score(y_valid, log_y_pred)\n",
    "log_f1_score = f1_score(y_valid,log_y_pred)\n",
    "log_train_report = classification_report(y_valid, log_y_pred)\n",
    "\n",
    "print(f'Accuracy: {log_train_accuracy}')\n",
    "print(f'F1 Score: {log_f1_score}')\n",
    "print(log_train_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x224f14e0130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepath(f):\n",
    "    d = os.path.join(os.path.dirname(os.getcwd()), 'processed_data', f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training and test datasets\n",
    "train_data = pd.read_csv(filepath(\"final_training_set.csv\"))\n",
    "test_data = pd.read_csv(filepath(\"final_test_set.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = train_data[\"ClaimID\"]\n",
    "train_data = train_data.drop(\n",
    "    [\"ClaimID\"],\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_encode(df, col):\n",
    "    \"\"\"\n",
    "    Return dataset including the minmax encoded column and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be numeric\n",
    "    \"\"\"\n",
    "\n",
    "    maxx = df[col].max()\n",
    "    minx = df[col].min()\n",
    "    out = list(map(lambda x: (x-minx)/(maxx-minx), df[col]))\n",
    "    new_colname = col + \"_minmax\"\n",
    "    df[new_colname] = out\n",
    "    return df.drop(\n",
    "        [col],\n",
    "        axis = 1\n",
    "    )\n",
    "\n",
    "def one_hot_encode(df, col):\n",
    "    \"\"\"\n",
    "    Returns the dataset including the one hot encoded columns and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be a Series that represents a categorical variable\n",
    "    \"\"\"\n",
    "    ohe_cols = pd.get_dummies(df[col], prefix = col)\n",
    "    output = pd.concat(\n",
    "        [df, ohe_cols],\n",
    "        axis = 1,\n",
    "    ).drop(\n",
    "        [col],\n",
    "        axis = 1\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def frequency_encode(df, col):\n",
    "    \"\"\"\n",
    "    Returns the dataset including the frequency encoded column and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be a Series that represents a categorical variable with high cardinality\n",
    "    \"\"\"\n",
    "    val_counts = df[col].value_counts().to_dict()\n",
    "    total = len(df[col])\n",
    "    out = []\n",
    "    for x in df[col]:\n",
    "        out.append(val_counts[x]/total)\n",
    "    new_colname = col + '_freq'\n",
    "    df[new_colname] = out\n",
    "    df.drop(\n",
    "        [col],\n",
    "        axis = 1,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procedure_1 has been removed as it is constant\n",
      "procedure_2 has been removed as it is constant\n",
      "procedure_3 has been removed as it is constant\n"
     ]
    }
   ],
   "source": [
    "freq_encoded_cols = []\n",
    "ohe_cols = []\n",
    "num_cols = []\n",
    "unique_threshold = 30\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].nunique() == 1:\n",
    "        print(col,\"has been removed as it is constant\")\n",
    "        train_data.drop([col], axis=1, inplace=True)\n",
    "    elif train_data[col].nunique() == 2: # Binary columns\n",
    "        continue\n",
    "    elif train_data[col].dtype in ['int64','float64']:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "        num_cols.append(col)\n",
    "    elif train_data[col].nunique() > unique_threshold:\n",
    "        freq_encoded_cols.append(col)\n",
    "    elif 2 < train_data[col].nunique() <= unique_threshold:\n",
    "        ohe_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:\n",
    "    if col in num_cols:\n",
    "        train_data = minmax_encode(train_data, col)\n",
    "    elif col in ohe_cols:\n",
    "        train_data = one_hot_encode(train_data, col)\n",
    "    elif col in freq_encoded_cols:\n",
    "        try:\n",
    "            train_data = frequency_encode(train_data, col)\n",
    "        except:\n",
    "            print(col)\n",
    "    elif train_data[col].nunique() == 1:\n",
    "        train_data.drop(\n",
    "            [col],\n",
    "            axis = 1,\n",
    "            inplace = True\n",
    "        )\n",
    "    else:\n",
    "        train_data[col] = train_data[col].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[\"PotentialFraud\"]\n",
    "train_data.drop(\n",
    "    [\"PotentialFraud\"],\n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAN model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.output_dim),\n",
    "            nn.Sigmoid() # must be between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice, random, randint\n",
    "def generate_one_random_data():\n",
    "    categorical = [randint(0, 1) for _ in range(49)]\n",
    "    numerical = [random() for _ in range(11)]\n",
    "    return [*categorical, *numerical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_dim = 30\n",
    "generator = Generator(gen_input_dim, X_train.shape[1]).to(device=device)\n",
    "discriminator = Discriminator(X_train.shape[1], 1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "lr = 0.0001\n",
    "num_epochs = 10\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(X_train.values.astype(np.float64)).float(), torch.from_numpy(y_train.values.astype(np.float64)).float())\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss D.: 0.020049110054969788\tLoss G.: 5.391129016876221\n",
      "Epoch: 1\tLoss D.: 0.0541888065636158\tLoss G.: 6.2767839431762695\n",
      "Epoch: 2\tLoss D.: 0.09054140746593475\tLoss G.: 10.610733032226562\n",
      "Epoch: 3\tLoss D.: 0.025492262095212936\tLoss G.: 7.796809196472168\n",
      "Epoch: 4\tLoss D.: 0.08897071331739426\tLoss G.: 5.00631046295166\n",
      "Epoch: 5\tLoss D.: 0.0001852016430348158\tLoss G.: 8.411347389221191\n",
      "Epoch: 6\tLoss D.: 0.003181146690621972\tLoss G.: 5.727866172790527\n",
      "Epoch: 7\tLoss D.: 0.0005961034912616014\tLoss G.: 8.99804401397705\n",
      "Epoch: 8\tLoss D.: 0.006344071589410305\tLoss G.: 6.71708869934082\n",
      "Epoch: 9\tLoss D.: 0.00237495475448668\tLoss G.: 5.706980228424072\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        real_samples = real_samples.to(device=device)\n",
    "        real_samples_labels = torch.ones((batch_size, 1)).to(\n",
    "            device=device\n",
    "        )\n",
    "        latent_space_samples = torch.randn((batch_size, gen_input_dim)).to(\n",
    "            device=device\n",
    "        )\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1)).to(\n",
    "            device=device\n",
    "        )\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels\n",
    "        )\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        latent_space_samples = torch.randn((batch_size, gen_input_dim)).to(\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        if n == X_train.shape[0]//batch_size - 1:\n",
    "            print(f\"Epoch: {epoch}\\tLoss D.: {loss_discriminator}\\tLoss G.: {loss_generator}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.from_numpy(X_train.values.astype(np.float64)).float()\n",
    "dxt = discriminator(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986228301176976\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of discriminator classifying a real training sample as real\n",
    "y_pred = [round(dxt[i][0].item()) for i in range(dxt.shape[0])]\n",
    "print(sum(y_pred)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.from_numpy(X_valid.values.astype(np.float64)).float()\n",
    "dxt = discriminator(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9983966751162187\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of discriminator classifying a real validation sample as real\n",
    "y_pred = [round(dxt[i][0].item()) for i in range(dxt.shape[0])]\n",
    "print(sum(y_pred)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of discriminator classifying a fake validation sample as fake\n",
    "generated_samples = generator(torch.randn((100_000, gen_input_dim)).to(device=device)).detach()\n",
    "dxt = discriminator(generated_samples)\n",
    "y_pred = [1-round(dxt[i][0].item()) for i in range(dxt.shape[0])]\n",
    "print(sum(y_pred)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+00, 6.8942e-09, 1.5068e-05, 1.0000e+00, 1.0000e+00,\n",
       "        8.1041e-12, 7.1588e-07, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.6914e-08,\n",
       "        1.4657e-07, 9.9994e-01, 1.0000e+00, 6.4534e-09, 1.2569e-09, 9.9997e-01,\n",
       "        9.0677e-07, 1.9429e-03, 9.9999e-01, 1.0000e+00, 2.0189e-09, 3.8514e-13,\n",
       "        1.7070e-07, 1.3709e-29, 2.1276e-07, 6.7969e-08, 1.5389e-19, 1.3059e-29,\n",
       "        1.0700e-06, 9.9923e-01, 1.1826e-10, 7.3786e-08, 1.2279e-29, 1.5390e-21,\n",
       "        9.9945e-01, 3.5383e-08, 1.5852e-26, 9.9998e-01, 2.6171e-13, 9.3826e-10,\n",
       "        2.1972e-25, 1.1349e-30, 2.7508e-24, 1.0446e-30, 1.5430e-16, 4.7654e-13,\n",
       "        1.2049e-10, 1.3134e-28, 1.0037e-15, 9.9978e-01, 8.5892e-10, 5.8571e-08,\n",
       "        9.6560e-09, 1.4167e-18, 5.8883e-18, 2.3362e-06, 1.1612e-12, 1.1260e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_samples[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

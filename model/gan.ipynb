{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e0f0d31130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepath(f):\n",
    "    d = os.path.join(os.path.dirname(os.getcwd()), 'processed_data', f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training and test datasets\n",
    "train_data = pd.read_csv(filepath(\"final_training_set.csv\"))\n",
    "test_data = pd.read_csv(filepath(\"final_test_set.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = train_data[\"ClaimID\"]\n",
    "train_data = train_data.drop(\n",
    "    [\"ClaimID\"],\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_encode(df, col):\n",
    "    \"\"\"\n",
    "    Return dataset including the minmax encoded column and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be numeric\n",
    "    \"\"\"\n",
    "\n",
    "    maxx = df[col].max()\n",
    "    minx = df[col].min()\n",
    "    out = list(map(lambda x: (x-minx)/(maxx-minx), df[col]))\n",
    "    new_colname = col + \"_minmax\"\n",
    "    df[new_colname] = out\n",
    "    return df.drop(\n",
    "        [col],\n",
    "        axis = 1\n",
    "    )\n",
    "\n",
    "def one_hot_encode(df, col):\n",
    "    \"\"\"\n",
    "    Returns the dataset including the one hot encoded columns and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be a Series that represents a categorical variable\n",
    "    \"\"\"\n",
    "    ohe_cols = pd.get_dummies(df[col], prefix = col)\n",
    "    output = pd.concat(\n",
    "        [df, ohe_cols],\n",
    "        axis = 1,\n",
    "    ).drop(\n",
    "        [col],\n",
    "        axis = 1\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def frequency_encode(df, col):\n",
    "    \"\"\"\n",
    "    Returns the dataset including the frequency encoded column and excluding the original column\n",
    "\n",
    "    Constraints:\n",
    "    - col must be a String\n",
    "    - df must be a Pandas Dataframe\n",
    "    - df[col] must be a Series that represents a categorical variable with high cardinality\n",
    "    \"\"\"\n",
    "    val_counts = df[col].value_counts().to_dict()\n",
    "    total = len(col)\n",
    "    out = []\n",
    "    for x in df[col]:\n",
    "        out.append(val_counts[x]/total)\n",
    "    new_colname = col + '_freq'\n",
    "    df[new_colname] = out\n",
    "    df.drop(\n",
    "        [col],\n",
    "        axis = 1,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procedure_1 has been removed as it is constant\n",
      "procedure_2 has been removed as it is constant\n",
      "procedure_3 has been removed as it is constant\n"
     ]
    }
   ],
   "source": [
    "freq_encoded_cols = []\n",
    "ohe_cols = []\n",
    "num_cols = []\n",
    "unique_threshold = 30\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].nunique() == 1:\n",
    "        print(col,\"has been removed as it is constant\")\n",
    "        train_data.drop([col], axis=1, inplace=True)\n",
    "    elif train_data[col].nunique() == 2: # Binary columns\n",
    "        continue\n",
    "    elif train_data[col].dtype in ['int64','float64']:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "        num_cols.append(col)\n",
    "    elif train_data[col].nunique() > unique_threshold:\n",
    "        freq_encoded_cols.append(col)\n",
    "    elif 2 < train_data[col].nunique() <= unique_threshold:\n",
    "        ohe_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:\n",
    "    if col in num_cols:\n",
    "        train_data = minmax_encode(train_data, col)\n",
    "    elif col in ohe_cols:\n",
    "        train_data = one_hot_encode(train_data, col)\n",
    "    elif col in freq_encoded_cols:\n",
    "        try:\n",
    "            train_data = frequency_encode(train_data, col)\n",
    "        except:\n",
    "            print(col)\n",
    "    elif train_data[col].nunique() == 1:\n",
    "        train_data.drop(\n",
    "            [col],\n",
    "            axis = 1,\n",
    "            inplace = True\n",
    "        )\n",
    "    else:\n",
    "        train_data[col] = train_data[col].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[\"PotentialFraud\"]\n",
    "train_data.drop(\n",
    "    [\"PotentialFraud\"],\n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAN model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_dim = 30\n",
    "generator = Generator(gen_input_dim, X_train.shape[1]).to(device=device)\n",
    "discriminator = Discriminator(X_train.shape[1], 1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "lr = 0.0001\n",
    "num_epochs = 5\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.from_numpy(X_train.values.astype(np.float64)).float(), torch.from_numpy(y_train.values.astype(np.float64)).float())\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss D.: 0.12065199762582779\tLoss G.: 2.2202467918395996\n",
      "Epoch: 1\tLoss D.: 0.07293358445167542\tLoss G.: 3.4020140171051025\n",
      "Epoch: 2\tLoss D.: 0.04544437676668167\tLoss G.: 3.8764607906341553\n",
      "Epoch: 3\tLoss D.: 0.05878283828496933\tLoss G.: 3.889295816421509\n",
      "Epoch: 4\tLoss D.: 0.04253915324807167\tLoss G.: 4.282777786254883\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        real_samples = real_samples.to(device=device)\n",
    "        real_samples_labels = torch.ones((batch_size, 1)).to(\n",
    "            device=device\n",
    "        )\n",
    "        latent_space_samples = torch.randn((batch_size, gen_input_dim)).to(\n",
    "            device=device\n",
    "        )\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1)).to(\n",
    "            device=device\n",
    "        )\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels\n",
    "        )\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        latent_space_samples = torch.randn((batch_size, gen_input_dim)).to(\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        if n == X_train.shape[0]//batch_size - 1:\n",
    "            print(f\"Epoch: {epoch}\\tLoss D.: {loss_discriminator}\\tLoss G.: {loss_generator}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        ...,\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0035]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = torch.from_numpy(X_train.values.astype(np.float64)).float()\n",
    "discriminator(xt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
